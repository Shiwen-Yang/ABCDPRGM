{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from src import visualize_latent_space as vls\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#########################################################################################################################################################################\n",
    "#########################################################################################################################################################################\n",
    "#########################################################################################################################################################################\n",
    "#Riemannian Gradient Descent on O(p)\n",
    "#########################################################################################################################################################################\n",
    "#########################################################################################################################################################################\n",
    "#########################################################################################################################################################################\n",
    "\n",
    "class Op_Riemannian_GD:\n",
    "    \n",
    "    \"\"\" \n",
    "    \n",
    "    Given some data, find an orthogonal transforamtion that put it into the standard simplex using Riemannian gradient descent on Op, the orthogonal group of dimension p. \n",
    "    \n",
    "    Args:\n",
    "        data (torch.tensor of dimension n by p): under our settings, it is the raw ASE that we cannot use to estimate the model parameter\n",
    "        tolerace (float): stopping criterion for the gradient descent\n",
    "        mode (string): either \"softplus\" or \"relu\", it defines the type of penalty function used\n",
    "        softplus_parameter (float): bigger parameter means that the softplus function looks more like the relu function, i.e. less smooth penalty\n",
    "    \n",
    "    Attributes:\n",
    "        relu_loss: loss under the relu penalty\n",
    "        softplus_loss: loss under the softplus penalty\n",
    "        align_mat: the desired orthogonal transformation\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, data, initialization = None, softplus_parameter = 5, tolerance = 0.01):\n",
    "\n",
    "        self.data = data\n",
    "        self.tolerance = tolerance\n",
    "        self.initialization = initialization\n",
    "        self.smoothing = softplus_parameter\n",
    "        self.softplus_loss = self.simplex_loss_softplus(self.data, self.smoothing)\n",
    "        self.align_mat = self.GD_Armijo()\n",
    "\n",
    "    def update(self, mode = None, smoothing = None, tolerance = None):\n",
    "        if mode is not None:\n",
    "            self.mode = mode\n",
    "        if smoothing is not None:\n",
    "            self.smoothing = smoothing\n",
    "        if tolerance is not None:\n",
    "            self.tolerance = tolerance\n",
    "        self.align_mat = self.GD_Armijo\n",
    "\n",
    "    @staticmethod\n",
    "    def cyclic_permutation_matrices(p):\n",
    "\n",
    "        # Start with the identity matrix of size p x p\n",
    "        identity_matrix = torch.eye(p)\n",
    "        \n",
    "        # Initialize an empty tensor to hold the result (p^2 x p)\n",
    "        result = torch.zeros((p * p, p), dtype=identity_matrix.dtype)\n",
    "        \n",
    "        # Loop through each cyclic permutation\n",
    "        for i in range(p):\n",
    "            # Apply the i-th cyclic permutation (rotate rows of the identity matrix)\n",
    "            permuted_matrix = torch.roll(identity_matrix, shifts=i, dims=0)\n",
    "            \n",
    "            # Stack the permuted matrix in the result\n",
    "            result[i * p:(i + 1) * p, :] = permuted_matrix\n",
    "        \n",
    "        return result\n",
    "        \n",
    "\n",
    "    \n",
    "    @staticmethod\n",
    "    def simplex_loss_softplus(data_set, transformation, smoothing):\n",
    "        \n",
    "        \"\"\" \n",
    "        \n",
    "        Same thing as the simplex_loss_relu function, but replacing the ReLU function with a softplus function with parameter smoothing. \n",
    "        The softplus function with parameter beta is defined to be:\n",
    "        \n",
    "        softplus(x, beta) = log(1 + exp(x * beta))/beta\n",
    "        \n",
    "        \"\"\"\n",
    "\n",
    "        X = data_set\n",
    "        n, p = X.shape\n",
    "\n",
    "        Ip = torch.eye(p)\n",
    "        permu = Op_Riemannian_GD.cyclic_permutation_matrices(p)\n",
    "\n",
    "        Z = torch.kron(Ip, X.matmul(transformation)).matmul(permu)\n",
    "        \n",
    "        mu = smoothing\n",
    "\n",
    "        softplus = torch.nn.Softplus(beta = mu)\n",
    "\n",
    "        negativity_loss = torch.sum(softplus(-Z))\n",
    "\n",
    "        row_sum_minus_1 = torch.sum(Z, dim = 1) - 1\n",
    "        simp_loss = torch.sum(softplus(row_sum_minus_1))\n",
    "\n",
    "        return(negativity_loss + simp_loss)\n",
    "    \n",
    "    def deriv_W_softplus(self, W):\n",
    "\n",
    "        X = self.data\n",
    "        n, p = X.shape\n",
    "        mu = self.smoothing\n",
    "        W.requires_grad_(True)\n",
    "\n",
    "        loss = Op_Riemannian_GD.simplex_loss_softplus(X, W, mu)\n",
    "        loss.backward()\n",
    "        return(W.grad)\n",
    "            \n",
    "  \n",
    "    # def deriv_W_softplus(self, W):\n",
    "        \n",
    "    #     \"\"\" \n",
    "    #     The derivative of the softplus function, L(X*W, mu) with respect to W, the orthogonal transformation \n",
    "    #     \"\"\"\n",
    "\n",
    "    #     X = self.data\n",
    "    #     n, p = X.shape\n",
    "        \n",
    "    #     mu = self.smoothing\n",
    "\n",
    "    #     T0 = torch.exp(-mu* X @ W)\n",
    "    #     deriv_neg = -X.T @ (T0/(1 + T0))\n",
    "\n",
    "    #     row_sum_minus_1 = torch.sum(X @ W, dim = 1) - 1\n",
    "\n",
    "    #     T1 = torch.exp(mu * row_sum_minus_1.unsqueeze(dim = 1))\n",
    "    #     deriv_simp = X.T @ (T1/(1 + T1)) @ torch.ones((1, p))\n",
    "\n",
    "    #     return(deriv_neg + deriv_simp)\n",
    "    \n",
    "    def proj_skew_sym_at_W(self, M, W):\n",
    "        \n",
    "        \"\"\" \n",
    "        Projection of M to the tangent space of Op at W\n",
    "        \"\"\"\n",
    "\n",
    "        projection = W @ (W.T @ M - M.T @ W)/2\n",
    "\n",
    "        return(projection)\n",
    "\n",
    "    def matrix_exp_at_W(self, xi, W):\n",
    "        \n",
    "        \"\"\" \n",
    "        The retractiom, it takes xi, the computed gradient step, and map it onto Op along a geodesic that starts at W\n",
    "        \"\"\"\n",
    "\n",
    "        Exp_w_xi = W @ torch.matrix_exp(W.T @ xi)\n",
    "\n",
    "        return(Exp_w_xi)\n",
    "    \n",
    "    def GD_one_step(self, prev_position, step):\n",
    "        \n",
    "        \"\"\" \n",
    "        Given the current orthogonal transformation, and a step size, take a graident descent step, and get another orthogonal transformation\n",
    "        \"\"\"\n",
    "        \n",
    "        W_old = prev_position\n",
    "\n",
    "        W_old = W_old * torch.sqrt(1/(W_old @ W_old.T)[0,0])\n",
    "        \n",
    "        euclid_deriv = self.deriv_W_softplus(W_old)\n",
    "        \n",
    "        tangent_deriv = self.proj_skew_sym_at_W(euclid_deriv, W_old)\n",
    "\n",
    "        W_new = self.matrix_exp_at_W(-step*tangent_deriv, W_old)\n",
    "        \n",
    "        return(W_new)\n",
    "    \n",
    "    def GD_Armijo(self):\n",
    "        \n",
    "        \"\"\" \n",
    "        Backtracking line search but uses the riemannian gradient instead\n",
    "        \"\"\"\n",
    "\n",
    "        X = self.data\n",
    "        n, p = X.shape\n",
    "\n",
    "        if self.initialization is not None:\n",
    "            W = self.initialization\n",
    "        else: \n",
    "            W = torch.eye(p)\n",
    "\n",
    "        grad = self.deriv_W_softplus\n",
    "        cost = partial(self.simplex_loss_softplus, smoothing = self.smoothing)\n",
    "        \n",
    "        b = 0.1; sigma = 0.1\n",
    "        max_iter = 200 * p\n",
    "\n",
    "        iter = 1\n",
    "        go = True\n",
    "        while go:\n",
    "            \n",
    "            t = 0.001\n",
    "            k = 1\n",
    "            while (cost(X @ self.GD_one_step(W, t)) > cost(X @ W) - sigma * t * torch.norm(grad(W))):\n",
    "                t = t * (b**k)\n",
    "                k += 1\n",
    "                if k > 10:\n",
    "                    break\n",
    "\n",
    "\n",
    "            W = self.GD_one_step(W, t)\n",
    "            jump = sigma * t * torch.norm(grad(W))\n",
    "\n",
    "            go = (torch.norm(grad(W)) > self.tolerance) & (jump > 10e-8) & (iter < max_iter)\n",
    "            iter += 1\n",
    "\n",
    "        return(W)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
