{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from src import visualize_latent_space as vls\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#########################################################################################################################################################################\n",
    "#########################################################################################################################################################################\n",
    "#########################################################################################################################################################################\n",
    "#Riemannian Gradient Descent on O(p)\n",
    "#########################################################################################################################################################################\n",
    "#########################################################################################################################################################################\n",
    "#########################################################################################################################################################################\n",
    "\n",
    "class Op_Riemannian_GD:\n",
    "    \n",
    "    \"\"\" \n",
    "    \n",
    "    Given some data, find an orthogonal transforamtion that put it into the standard simplex using Riemannian gradient descent on Op, the orthogonal group of dimension p. \n",
    "    \n",
    "    Args:\n",
    "        data (torch.tensor of dimension n by p): under our settings, it is the raw ASE that we cannot use to estimate the model parameter\n",
    "        tolerace (float): stopping criterion for the gradient descent\n",
    "        mode (string): either \"softplus\" or \"relu\", it defines the type of penalty function used\n",
    "        softplus_parameter (float): bigger parameter means that the softplus function looks more like the relu function, i.e. less smooth penalty\n",
    "    \n",
    "    Attributes:\n",
    "        relu_loss: loss under the relu penalty\n",
    "        softplus_loss: loss under the softplus penalty\n",
    "        align_mat: the desired orthogonal transformation\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, data, initialization = None, softplus_parameter = 5, tolerance = 0.01):\n",
    "\n",
    "        self.data = data\n",
    "        self.tolerance = tolerance\n",
    "        self.initialization = initialization\n",
    "        self.smoothing = softplus_parameter\n",
    "        self.softplus_loss = self.simplex_loss_softplus(self.data, self.smoothing)\n",
    "        self.align_mat = self.GD_Armijo()\n",
    "\n",
    "    def update(self, mode = None, smoothing = None, tolerance = None):\n",
    "        if mode is not None:\n",
    "            self.mode = mode\n",
    "        if smoothing is not None:\n",
    "            self.smoothing = smoothing\n",
    "        if tolerance is not None:\n",
    "            self.tolerance = tolerance\n",
    "        self.align_mat = self.GD_Armijo\n",
    "\n",
    "    @staticmethod\n",
    "    def cyclic_permutation_matrices(p):\n",
    "\n",
    "        # Start with the identity matrix of size p x p\n",
    "        identity_matrix = torch.eye(p)\n",
    "        \n",
    "        # Initialize an empty tensor to hold the result (p^2 x p)\n",
    "        result = torch.zeros((p * p, p), dtype=identity_matrix.dtype)\n",
    "        \n",
    "        # Loop through each cyclic permutation\n",
    "        for i in range(p):\n",
    "            # Apply the i-th cyclic permutation (rotate rows of the identity matrix)\n",
    "            permuted_matrix = torch.roll(identity_matrix, shifts=i, dims=0)\n",
    "            \n",
    "            # Stack the permuted matrix in the result\n",
    "            result[i * p:(i + 1) * p, :] = permuted_matrix\n",
    "        \n",
    "        return result\n",
    "        \n",
    "\n",
    "    \n",
    "    @staticmethod\n",
    "    def simplex_loss_softplus(data_set, transformation, smoothing):\n",
    "        \n",
    "        \"\"\" \n",
    "        \n",
    "        Same thing as the simplex_loss_relu function, but replacing the ReLU function with a softplus function with parameter smoothing. \n",
    "        The softplus function with parameter beta is defined to be:\n",
    "        \n",
    "        softplus(x, beta) = log(1 + exp(x * beta))/beta\n",
    "        \n",
    "        \"\"\"\n",
    "\n",
    "        X = data_set\n",
    "        n, p = X.shape\n",
    "\n",
    "        Ip = torch.eye(p)\n",
    "        permu = Op_Riemannian_GD.cyclic_permutation_matrices(p)\n",
    "\n",
    "        Z = torch.kron(Ip, X.matmul(transformation)).matmul(permu)\n",
    "        \n",
    "        mu = smoothing\n",
    "\n",
    "        softplus = torch.nn.Softplus(beta = mu)\n",
    "\n",
    "        negativity_loss = torch.sum(softplus(-Z))\n",
    "\n",
    "        row_sum_minus_1 = torch.sum(Z, dim = 1) - 1\n",
    "        simp_loss = torch.sum(softplus(row_sum_minus_1))\n",
    "\n",
    "        return(negativity_loss + simp_loss)\n",
    "    \n",
    "    def deriv_W_softplus(self, W):\n",
    "\n",
    "        X = self.data\n",
    "        n, p = X.shape\n",
    "        mu = self.smoothing\n",
    "        W.requires_grad_(True)\n",
    "\n",
    "        loss = Op_Riemannian_GD.simplex_loss_softplus(X, W, mu)\n",
    "        loss.backward()\n",
    "        return(W.grad)\n",
    "            \n",
    "  \n",
    "    # def deriv_W_softplus(self, W):\n",
    "        \n",
    "    #     \"\"\" \n",
    "    #     The derivative of the softplus function, L(X*W, mu) with respect to W, the orthogonal transformation \n",
    "    #     \"\"\"\n",
    "\n",
    "    #     X = self.data\n",
    "    #     n, p = X.shape\n",
    "        \n",
    "    #     mu = self.smoothing\n",
    "\n",
    "    #     T0 = torch.exp(-mu* X @ W)\n",
    "    #     deriv_neg = -X.T @ (T0/(1 + T0))\n",
    "\n",
    "    #     row_sum_minus_1 = torch.sum(X @ W, dim = 1) - 1\n",
    "\n",
    "    #     T1 = torch.exp(mu * row_sum_minus_1.unsqueeze(dim = 1))\n",
    "    #     deriv_simp = X.T @ (T1/(1 + T1)) @ torch.ones((1, p))\n",
    "\n",
    "    #     return(deriv_neg + deriv_simp)\n",
    "    \n",
    "    def proj_skew_sym_at_W(self, M, W):\n",
    "        \n",
    "        \"\"\" \n",
    "        Projection of M to the tangent space of Op at W\n",
    "        \"\"\"\n",
    "\n",
    "        projection = W @ (W.T @ M - M.T @ W)/2\n",
    "\n",
    "        return(projection)\n",
    "\n",
    "    def matrix_exp_at_W(self, xi, W):\n",
    "        \n",
    "        \"\"\" \n",
    "        The retractiom, it takes xi, the computed gradient step, and map it onto Op along a geodesic that starts at W\n",
    "        \"\"\"\n",
    "\n",
    "        Exp_w_xi = W @ torch.matrix_exp(W.T @ xi)\n",
    "\n",
    "        return(Exp_w_xi)\n",
    "    \n",
    "    def GD_one_step(self, prev_position, step):\n",
    "        \n",
    "        \"\"\" \n",
    "        Given the current orthogonal transformation, and a step size, take a graident descent step, and get another orthogonal transformation\n",
    "        \"\"\"\n",
    "        \n",
    "        W_old = prev_position\n",
    "\n",
    "        W_old = W_old * torch.sqrt(1/(W_old @ W_old.T)[0,0])\n",
    "        \n",
    "        euclid_deriv = self.deriv_W_softplus(W_old)\n",
    "        \n",
    "        tangent_deriv = self.proj_skew_sym_at_W(euclid_deriv, W_old)\n",
    "\n",
    "        W_new = self.matrix_exp_at_W(-step*tangent_deriv, W_old)\n",
    "        \n",
    "        return(W_new)\n",
    "    \n",
    "    def GD_Armijo(self):\n",
    "        \n",
    "        \"\"\" \n",
    "        Backtracking line search but uses the riemannian gradient instead\n",
    "        \"\"\"\n",
    "\n",
    "        X = self.data\n",
    "        n, p = X.shape\n",
    "\n",
    "        if self.initialization is not None:\n",
    "            W = self.initialization\n",
    "        else: \n",
    "            W = torch.eye(p)\n",
    "\n",
    "        grad = self.deriv_W_softplus\n",
    "        cost = partial(self.simplex_loss_softplus, smoothing = self.smoothing)\n",
    "        \n",
    "        b = 0.1; sigma = 0.1\n",
    "        max_iter = 200 * p\n",
    "\n",
    "        iter = 1\n",
    "        go = True\n",
    "        while go:\n",
    "            \n",
    "            t = 0.001\n",
    "            k = 1\n",
    "            while (cost(X @ self.GD_one_step(W, t)) > cost(X @ W) - sigma * t * torch.norm(grad(W))):\n",
    "                t = t * (b**k)\n",
    "                k += 1\n",
    "                if k > 10:\n",
    "                    break\n",
    "\n",
    "\n",
    "            W = self.GD_one_step(W, t)\n",
    "            jump = sigma * t * torch.norm(grad(W))\n",
    "\n",
    "            go = (torch.norm(grad(W)) > self.tolerance) & (jump > 10e-8) & (iter < max_iter)\n",
    "            iter += 1\n",
    "\n",
    "        return(W)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from tqdm import tqdm as tm\n",
    "from src import Simulation as sim\n",
    "from src import Dir_Reg as DR\n",
    "from src import ABC_Reg_copy as ABC_Reg \n",
    "from src import Align\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "device = torch.device(\"mps\") if torch.backends.mps.is_available() else device\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we want to investigate what happens to our estimation, when we use the correct model, but misspecify the dimension. \n",
    "# 1. generate some synthetic data with 8 dimensions\n",
    "# 2. first do full oracle with the correct dimension to get base line\n",
    "# 3. then embed the graph in lower dimension, run regression, and then compare result to full oracle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(5)\n",
    "\n",
    "p, K, T, n = 6, 3, 2, 6000\n",
    "alpha_0 = torch.ones(K, p)*2\n",
    "# alpha_0 = torch.eye(p)*9 + 1\n",
    "\n",
    "model = sim.ABC(time = T,\n",
    "                nodes = n,\n",
    "                beta = [1, 1, -4, 5],\n",
    "                alpha_0 = alpha_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = ABC_Reg.est(two_lat_pos = model.synth_data['lat_pos'],\n",
    "                   two_adj_mat = model.synth_data['obs_adj'],\n",
    "                   groups = K,\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.9733,  0.8217, -4.2562,  4.3985])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp.specify_mode('OL', fit = True)\n",
    "DR.fit.proj_beta(temp.fitted.est_result[\"estimate\"], DR.fit.gen_constraint(p, True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "0.02\n",
      "0.03\n",
      "0.05\n",
      "0.07\n",
      "0.08\n",
      "0.1\n",
      "0.12\n",
      "0.13\n",
      "0.15\n",
      "0.17\n",
      "0.18\n",
      "0.2\n",
      "0.22\n",
      "0.23\n",
      "0.25\n",
      "0.27\n",
      "0.28\n",
      "0.3\n",
      "0.32\n",
      "0.33\n",
      "0.35\n",
      "0.37\n",
      "0.38\n",
      "0.4\n",
      "0.42\n",
      "0.43\n",
      "0.45\n",
      "0.47\n",
      "0.48\n",
      "0.5\n",
      "0.52\n",
      "0.53\n",
      "0.55\n",
      "0.57\n",
      "0.58\n",
      "0.6\n",
      "0.62\n",
      "0.63\n",
      "0.65\n"
     ]
    }
   ],
   "source": [
    "p, K, T = 6, 3, 2\n",
    "alpha_0 = torch.ones(K, p)*2\n",
    "\n",
    "model = sim.ABC(time = T,\n",
    "        nodes = 30,\n",
    "        beta = [1, 1, -4, 5],\n",
    "        alpha_0 = alpha_0)\n",
    "\n",
    "seed_list = list(range(60))\n",
    "n_list = list(range(1500, 6001, 1500))\n",
    "p0_list = list(range(2, 8))\n",
    "\n",
    "# Initialize an empty list to store results\n",
    "results = []\n",
    "for seed in seed_list:\n",
    "    torch.manual_seed(seed)\n",
    "    print(round(seed/60, 2))\n",
    "    for n in n_list:\n",
    "        for p0 in p0_list:\n",
    "            # Update model settings\n",
    "            model.update_settings(nodes = n)\n",
    "            # Initialize estimation\n",
    "            estimate = ABC_Reg.est(two_lat_pos = model.synth_data['lat_pos'],\n",
    "                    two_adj_mat = model.synth_data['obs_adj'],\n",
    "                    groups = K,\n",
    "                    )\n",
    "            # Perform estimation by specifying mode and embedding dimension p0\n",
    "            estimate.specify_mode('NO', fit = True, embed_dim = p0)\n",
    "            \n",
    "            # Compute beta_est and info_lost\n",
    "            beta_est = DR.fit.proj_beta(estimate.fitted.est_result[\"estimate\"], DR.fit.gen_constraint(p0+1, True)).tolist()\n",
    "            info_lost = estimate.fitted.est_result[\"info_lost\"]\n",
    "            \n",
    "            # Create a dictionary for the current iteration\n",
    "            result = {\n",
    "                'seed': seed,\n",
    "                'n': n,\n",
    "                'p0': p0,\n",
    "                'beta1': beta_est[0],\n",
    "                'beta2': beta_est[1],\n",
    "                'beta3': beta_est[2],\n",
    "                'beta4': beta_est[3],\n",
    "                'info_lost': info_lost  # Optional: Include if you want to store this value\n",
    "            }\n",
    "            # Append the result to the list\n",
    "            results.append(result)\n",
    "\n",
    "# Convert the list of dictionaries to a pandas DataFrame\n",
    "df = pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.032333333333333325\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([ 0.3957,  0.0590, -1.8203, 11.5494])"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_0 = 3\n",
    "temp.specify_mode('NO', fit = True, embed_dim = p_0)\n",
    "print(temp.fitted.est_result[\"info_lost\"])\n",
    "DR.fit.proj_beta(temp.fitted.est_result[\"estimate\"], DR.fit.gen_constraint(p_0+1, True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.09533333333333338\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([ 0.3947,  0.0186, -1.5651,  0.8212])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_0 = 4\n",
    "temp.specify_mode('NO', fit = True, embed_dim = p_0)\n",
    "print(temp.fitted.est_result[\"info_lost\"])\n",
    "DR.fit.proj_beta(temp.fitted.est_result[\"estimate\"], DR.fit.gen_constraint(p_0+1, True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2743333333333333\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([ 0.4642,  0.4226, -2.3290,  2.4509])"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_0 = 5\n",
    "temp.specify_mode('NO', fit = True, embed_dim = p_0)\n",
    "print(temp.fitted.est_result[\"info_lost\"])\n",
    "DR.fit.proj_beta(temp.fitted.est_result[\"estimate\"], DR.fit.gen_constraint(p_0+1, True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.43066666666666664\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([ 0.6828,  0.8133, -3.9691, -3.2112])"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_0 = 6\n",
    "temp.specify_mode('NO', fit = True, embed_dim = p_0)\n",
    "print(temp.fitted.est_result[\"info_lost\"])\n",
    "DR.fit.proj_beta(temp.fitted.est_result[\"estimate\"], DR.fit.gen_constraint(p_0+1, True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7296666666666667\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([ 0.9345,  1.1643, -3.5133,  5.0706])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_0 = 7\n",
    "temp.specify_mode('NO', fit = True, embed_dim = p_0)\n",
    "print(temp.fitted.est_result[\"info_lost\"])\n",
    "DR.fit.proj_beta(temp.fitted.est_result[\"estimate\"], DR.fit.gen_constraint(p_0+1, True))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
