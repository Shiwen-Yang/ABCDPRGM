{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.distributions.normal import Normal\n",
    "from torch.distributions.dirichlet import Dirichlet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_expec_loss_cond_epsilon(Z_i_star, V, sigma):\n",
    "    \"\"\"\n",
    "    Compute the given expression in PyTorch.\n",
    "    \n",
    "    Args:\n",
    "        Z_i_star (torch.Tensor): The Z_{i*} vector, shape (p,1).\n",
    "        V (torch.Tensor): The V matrix, shape (p,p).\n",
    "        sigma (float): The standard deviation (scalar).\n",
    "        \n",
    "    Returns:\n",
    "        torch.Tensor: The loss and derivative of the loss with respect to V\n",
    "    \"\"\"\n",
    "    # Initialize normal distribution for standard normal\n",
    "    normal = Normal(0, 1)\n",
    "\n",
    "    # Compute intermediate values\n",
    "    term = -Z_i_star.T @ V / sigma  # Shape: scalar\n",
    "    phi = normal.log_prob(term).exp()  # \\phi(x), scalar\n",
    "    Phi = normal.cdf(term)  # \\Phi(x), scalar\n",
    "\n",
    "    # Compute the element-wise operations\n",
    "    vec = (phi +  term * Phi) * sigma\n",
    "    \n",
    "    # Final computation\n",
    "    loss = torch.sum(vec)\n",
    "    deriv = -Z_i_star @ Phi\n",
    "    \n",
    "    return  loss, deriv\n",
    "\n",
    "def compute_raw_loss(Z_i_star, V, sigma):\n",
    "    \"\"\"\n",
    "    Compute the given loss in PyTorch.\n",
    "    \n",
    "    Args:\n",
    "        Z_i_star (torch.Tensor): The Z_{i*} vector, shape (p,1).\n",
    "        V (torch.Tensor): The V matrix, shape (p,p).\n",
    "        sigma (float): The standard deviation (scalar).\n",
    "        \n",
    "    Returns:\n",
    "        torch.Tensor: The result of the expression, shape (p,).\n",
    "    \"\"\"\n",
    "    p = Z_i_star.shape[0]\n",
    "    noise_gauss = Normal(0, sigma)\n",
    "    relu = nn.ReLU()\n",
    "    \n",
    "    epsilon = noise_gauss.sample((p,)).view(p,1)\n",
    "    loss = torch.sum(relu((- Z_i_star - epsilon).T @ V))\n",
    "    return loss\n",
    "    \n",
    "    \n",
    "def proj_Op_tangent_space_at_W(M, W):\n",
    "    \"\"\"\n",
    "    Projects a matrix M onto the tangent space of the orthogonal group O_p at W.\n",
    "    \n",
    "    Parameters:\n",
    "        M (torch.Tensor or np.ndarray): The matrix to be projected, of shape (p, p).\n",
    "        W (torch.Tensor or np.ndarray): A point in the orthogonal group O_p, of shape (p, p)\n",
    "                                         \n",
    "    Returns:\n",
    "        torch.Tensor or np.ndarray: The projection of M onto the tangent space of O_p at W,\n",
    "                                    of shape (p, p).\n",
    "    \"\"\"\n",
    "    proj = W @ (W.T @ M - M.T @ W) / 2\n",
    "    return proj\n",
    "    \n",
    "\n",
    "def sample_Z_i_star(p):\n",
    "    \"\"\"\n",
    "    Samples Z_i_star uniformly from the standard basis of R^p.\n",
    "    \n",
    "    Args:\n",
    "        p (int): Dimension of the space R^p.\n",
    "        \n",
    "    Returns:\n",
    "        torch.Tensor: A p-dimensional tensor representing the sampled standard basis vector.\n",
    "    \"\"\"\n",
    "    # Create an identity matrix of shape (p, p) where rows are standard basis vectors\n",
    "    standard_basis = torch.eye(p)\n",
    "    \n",
    "    # Randomly choose one row (basis vector) uniformly\n",
    "    idx = torch.randint(0, p, (1,)).item()\n",
    "    Z_i_star = standard_basis[idx]\n",
    "    \n",
    "    return Z_i_star\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.0276), tensor(0.4823))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example usage\n",
    "p = 2\n",
    "Z_i_star = sample_Z_i_star(p).view(p, 1)  # Example Z_{i*} vector (p=10)\n",
    "# Z_i_star = torch.randn(p).view(p, 1)  # Example Z_{i*} vector (p=10)\n",
    "V = torch.eye(p)        # Example V vector (p=10)\n",
    "sigma = 1.0                 # Example scalar sigma\n",
    "\n",
    "compute_raw_loss(Z_i_star, V, sigma),compute_expec_loss_cond_epsilon(Z_i_star, V, sigma)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To verify empirically that the expectation computation, and the derivative computation is correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the difference between the mean raw loss and the mean loss conditional on epsilon is -0.004753679037094116\n",
      "the difference between computed derivative and autograd is 1.4901161193847656e-08\n"
     ]
    }
   ],
   "source": [
    "n, p = 10000, 2\n",
    "sigma = 1.0 \n",
    "V = torch.eye(p) \n",
    "\n",
    "\n",
    "raw_loss = []\n",
    "conditional_epsilon = []\n",
    "\n",
    "for i in range(n):\n",
    "    Z_i_star = sample_Z_i_star(p).view(p, 1)\n",
    "    # Z_i_star = dir_distr.sample((1,)).view(p, 1)  # Example Z_{i*} vector (p=10)\n",
    "    conditional_epsilon.append(compute_expec_loss_cond_epsilon(Z_i_star, V, sigma)[0])\n",
    "    raw_loss.append(compute_raw_loss(Z_i_star, V, sigma))\n",
    "    \n",
    "# Difference in mean\n",
    "print(f\"the difference between the mean raw loss and the mean loss conditional on epsilon is {torch.mean(torch.tensor(conditional_epsilon)) - torch.mean(torch.tensor(raw_loss))}\" )\n",
    "\n",
    "# Variance of the two lists\n",
    "# print(torch.var(torch.tensor(conditional_epsilon)), torch.var(torch.tensor(raw_loss)))\n",
    "\n",
    "V = torch.eye(p) \n",
    "V.requires_grad_(True)\n",
    "result = compute_expec_loss_cond_epsilon(Z_i_star, V, sigma)\n",
    "result[0].backward()\n",
    "print(f\"the difference between computed derivative and autograd is {torch.norm(V.grad - result[1])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0011)\n"
     ]
    }
   ],
   "source": [
    "n, p = 10000, 10\n",
    "sigma = 0.1\n",
    "V = torch.eye(p) \n",
    "dir_distr = Dirichlet(torch.ones(p)*5)\n",
    "\n",
    "\n",
    "derivative_across_n = torch.zeros(p, p)\n",
    "\n",
    "for i in range(n):\n",
    "    # Z_i_star = sample_Z_i_star(p).view(p, 1)\n",
    "    Z_i_star = dir_distr.sample((1,)).view(p, 1)  # Example Z_{i*} vector (p=10)\n",
    "    derivative_across_n = derivative_across_n + compute_expec_loss_cond_epsilon(Z_i_star, V, sigma)[1]\n",
    "    \n",
    "avg_deriv = derivative_across_n/n\n",
    "\n",
    "proj_avg_deriv = proj_Op_tangent_space_at_W(avg_deriv, V)\n",
    "\n",
    "print(torch.norm(proj_Op_tangent_space_at_W(avg_deriv, V), p = \"fro\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
